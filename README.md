# AECF - Adaptive Ensemble CLIP Fusion

A production-ready, modular multi-modal learning framework implementing Adaptive Early Cross-modal Fusion for PyTorch.

## ğŸ¯ Overview

AECF addresses the challenge of effectively fusing multi-modal information (image, text, audio) in deep learning models through adaptive gating mechanisms and curriculum learning strategies.

## âœ¨ Key Features

- **ğŸ—ï¸ Modular Architecture**: Clean separation of concerns following SOLID principles
- **ğŸ”§ Type-Safe Configuration**: Comprehensive validation with `AECFConfig` dataclass
- **ğŸš€ Memory Efficient**: Optimized training pipeline with efficient metrics accumulation
- **ğŸ§ª Extensively Tested**: Comprehensive test suite with validation
- **ğŸ“š Well Documented**: Extensive examples and API documentation
- **ğŸ”„ Backward Compatible**: Smooth migration path from legacy implementations

## ğŸš€ Quick Start

### Installation

```bash
pip install -e .
```

### Basic Usage

```python
from aecf import create_aecf_model, test_model_forward

# Create model with sensible defaults
model = create_aecf_model(
    modalities=["image", "text"],
    task_type="classification",
    num_classes=80
)

# Validate functionality
result = test_model_forward(model, batch_size=8)
print(f"âœ… Model working: {result['success']}")
```

### Custom Configuration

```python
from aecf import AECFConfig, AECF_CLIP

config = AECFConfig(
    modalities=["image", "text", "audio"],
    task_type="regression",
    feat_dim=768,
    gate_hidden_dims=[1024, 512],
    learning_rate=5e-4,
    entropy_max_coeff=0.2,
    masking_strategy="entropy_min"
)

model = AECF_CLIP(config)
```

### Training Pipeline

```python
from aecf import AECFTrainer

trainer = AECFTrainer(
    config=config,
    logger_type="tensorboard"
)

results = trainer.fit(train_loader, val_loader)
```

## ğŸ“¦ Architecture

The AECF framework consists of focused modules:

1. **`config.py`** - Type-safe configuration with validation
2. **`components.py`** - Reusable components (gates, encoders, output heads)
3. **`model_refactored.py`** - Core model and PyTorch Lightning integration
4. **`losses.py`** - Modular loss functions with regularization
5. **`metrics.py`** - Comprehensive metrics for evaluation
6. **`training.py`** - Production-ready training pipeline
7. **`datasets.py`** - Data loading and preprocessing utilities
8. **`utils.py`** - Common utility functions

## ğŸ§ª Testing

Run the comprehensive test suite:

```bash
python -m pytest tests/ -v
```

## ğŸ“š Examples

Explore comprehensive usage examples:

```bash
python examples/comprehensive_examples.py
python examples/legacy_ablation.py  # Legacy compatibility
```

## ğŸ”„ Migration from Legacy Code

The framework provides seamless backward compatibility:

```python
# Convert old dict-based configs
legacy_config = {"lr": 1e-4, "gate_lr": 1e-3, "modalities": ["image", "text"]}
new_config = AECFConfig.from_dict(legacy_config)

# Use legacy model if needed
from aecf import AECF_CLIP_Legacy
legacy_model = AECF_CLIP_Legacy(legacy_config)
```

## ğŸ“Š Performance

The refactored system provides:
- **81% reduction** in main class size (805 â†’ 150 lines)
- **8x modularity** improvement (1 â†’ 8 focused modules)
- **Complete memory management** (fixed accumulation issues)
- **Comprehensive validation** (runtime â†’ compile-time error prevention)

## ğŸ† Production-Ready Framework

This codebase demonstrates the transformation from monolithic research code to production-ready PyTorch module following industry best practices.
