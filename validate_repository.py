#!/usr/bin/env python3
"""
Final validation script for the cleaned AECF repository.

This script validates that all components are working correctly
after the repository cleanup and refactoring.
"""

import sys
import traceback
from pathlib import Path

def test_imports():
    """Test that all imports work correctly."""
    print("üîç Testing imports...")
    
    try:
        # Core imports
        from aecf import (
            AECFConfig, AECF_CLIP, create_aecf_model, 
            AECFTrainer, test_model_forward
        )
        print("  ‚úÖ Core imports successful")
        
        # Component imports
        from aecf import (
            AdaptiveGate, CurriculumMasker, EncoderFactory, 
            OutputHeadFactory, AECFLoss, AECFMetrics
        )
        print("  ‚úÖ Component imports successful")
        
        # Utility imports
        from aecf import (
            validate_feature_dict, normalize_features, 
            MetricsAccumulator
        )
        print("  ‚úÖ Utility imports successful")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Import failed: {e}")
        traceback.print_exc()
        return False

def test_configuration():
    """Test configuration system."""
    print("\nüîß Testing configuration system...")
    
    try:
        from aecf import AECFConfig, create_default_config
        
        # Test default config
        config = create_default_config()
        print(f"  ‚úÖ Default config: {config.modalities}")
        
        # Test custom config
        custom_config = AECFConfig(
            modalities=["image", "text", "audio"],
            task_type="regression",
            num_classes=1,
            feat_dim=768
        )
        print(f"  ‚úÖ Custom config: {custom_config.task_type}")
        
        # Test legacy conversion
        legacy_dict = {"lr": 1e-4, "gate_lr": 1e-3, "modalities": ["image", "text"]}
        converted = AECFConfig.from_dict(legacy_dict)
        print(f"  ‚úÖ Legacy conversion: {converted.learning_rate}")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Configuration test failed: {e}")
        traceback.print_exc()
        return False

def test_model_functionality():
    """Test model creation and functionality."""
    print("\nü§ñ Testing model functionality...")
    
    try:
        import torch
        from aecf import create_aecf_model, test_model_forward
        
        # Create model
        model = create_aecf_model(
            modalities=["image", "text"],
            task_type="classification",
            num_classes=80
        )
        print(f"  ‚úÖ Model created successfully")
        
        # Test forward pass
        result = test_model_forward(model, batch_size=4)
        if result["success"]:
            print(f"  ‚úÖ Forward pass successful: {result['logits_shape']}")
        else:
            print(f"  ‚ùå Forward pass failed: {result['error']}")
            return False
        
        # Test model info
        info = model.get_model_info()
        print(f"  ‚úÖ Model info: {info['total_parameters']} parameters")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Model test failed: {e}")
        traceback.print_exc()
        return False

def test_loss_and_metrics():
    """Test loss and metrics functionality."""
    print("\nüìä Testing loss and metrics...")
    
    try:
        import torch
        from aecf import AECFConfig, AECFLoss, AECFMetrics
        
        config = AECFConfig()
        
        # Test loss function
        loss_fn = AECFLoss(config)
        dummy_logits = torch.randn(4, 80)
        dummy_targets = torch.randint(0, 2, (4, 80)).float()
        dummy_weights = torch.softmax(torch.randn(4, 2), dim=1)
        
        total_loss, components = loss_fn(dummy_logits, dummy_targets, dummy_weights)
        print(f"  ‚úÖ Loss computation: {total_loss.item():.4f}")
        
        # Test metrics
        metrics = AECFMetrics.compute_classification_metrics(dummy_logits, dummy_targets)
        print(f"  ‚úÖ Metrics computation: mAP={metrics['map'].item():.4f}")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Loss/metrics test failed: {e}")
        traceback.print_exc()
        return False

def test_memory_management():
    """Test memory management functionality."""
    print("\nüíæ Testing memory management...")
    
    try:
        from aecf import MetricsAccumulator
        import torch
        
        # Test metrics accumulator
        acc = MetricsAccumulator()
        
        for i in range(5):
            metrics = {
                "loss": torch.tensor(0.5 + i * 0.1),
                "accuracy": torch.tensor(0.8 - i * 0.05)
            }
            acc.update(metrics)
        
        final_metrics = acc.compute()
        print(f"  ‚úÖ Memory-efficient accumulation: avg_loss={final_metrics['loss']:.3f}")
        
        # Test reset
        acc.reset()
        empty_metrics = acc.compute()
        if len(empty_metrics) == 0:
            print("  ‚úÖ Metrics accumulator reset successful")
        else:
            print("  ‚ùå Metrics accumulator reset failed")
            return False
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Memory management test failed: {e}")
        traceback.print_exc()
        return False

def test_examples():
    """Test that examples can be imported."""
    print("\nüìö Testing examples...")
    
    try:
        # Test that example files exist and can be imported
        examples_dir = Path("examples")
        if examples_dir.exists():
            example_files = list(examples_dir.glob("*.py"))
            print(f"  ‚úÖ Found {len(example_files)} example files")
            
            # Test importing comprehensive examples
            sys.path.insert(0, str(examples_dir))
            try:
                import comprehensive_examples
                print("  ‚úÖ Comprehensive examples importable")
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Comprehensive examples import warning: {e}")
            
        return True
        
    except Exception as e:
        print(f"  ‚ùå Examples test failed: {e}")
        return False

def validate_repository_structure():
    """Validate the repository structure."""
    print("\nüìÅ Validating repository structure...")
    
    required_files = [
        "README.md",
        "requirements.txt",
        "setup.py",
        "pyproject.toml",
        "MANIFEST.in",
        "aecf/__init__.py",
        "aecf/config.py",
        "aecf/components.py",
        "aecf/model_refactored.py",
        "aecf/losses.py",
        "aecf/metrics.py",
        "aecf/training.py",
        "examples/comprehensive_examples.py",
        "tests/test_comprehensive.py"
    ]
    
    missing_files = []
    for file_path in required_files:
        if not Path(file_path).exists():
            missing_files.append(file_path)
    
    if missing_files:
        print(f"  ‚ùå Missing files: {missing_files}")
        return False
    else:
        print(f"  ‚úÖ All {len(required_files)} required files present")
        return True

def main():
    """Run all validation tests."""
    print("üéØ AECF REPOSITORY VALIDATION")
    print("=" * 50)
    
    tests = [
        ("Repository Structure", validate_repository_structure),
        ("Imports", test_imports),
        ("Configuration", test_configuration),
        ("Model Functionality", test_model_functionality),
        ("Loss and Metrics", test_loss_and_metrics),
        ("Memory Management", test_memory_management),
        ("Examples", test_examples),
    ]
    
    results = []
    for test_name, test_func in tests:
        success = test_func()
        results.append((test_name, success))
    
    print("\n" + "=" * 50)
    print("üìã VALIDATION RESULTS")
    print("=" * 50)
    
    all_passed = True
    for test_name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{test_name:.<30} {status}")
        if not success:
            all_passed = False
    
    print("\n" + "=" * 50)
    if all_passed:
        print("üéâ ALL TESTS PASSED!")
        print("üèÜ Repository is clean and production-ready!")
        print("‚ú® AECF refactoring complete and validated!")
    else:
        print("‚ùå Some tests failed. Please review the output above.")
        return False
    
    print("=" * 50)
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
